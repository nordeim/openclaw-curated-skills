#!/usr/bin/env python3
import json
import argparse
import re
import os
from datetime import datetime
from typing import List, Dict, Tuple
from collections import Counter

class DataCleaner:
    def __init__(self):
        self.emoticon_pattern = re.compile(r'[\U00010000-\U0010ffff]')
        self.spam_keywords = ['å¹¿å‘Š', 'æ¨å¹¿', 'åŠ å¾®ä¿¡', 'å…¼èŒ', 'èµšé’±', 'ä»£è´­', 'ä¼˜æƒ ', 'æŠ˜æ‰£']
        self.min_content_length = 5
        self.similarity_threshold = 0.8
    
    def clean_text(self, text: str) -> str:
        text = self.emoticon_pattern.sub('', text)
        text = re.sub(r'[^\w\s\u4e00-\u9fff,.!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', '', text)
        text = text.strip()
        return text
    
    def is_spam(self, text: str) -> bool:
        return any(keyword in text for keyword in self.spam_keywords)
    
    def is_valid(self, text: str) -> bool:
        if not text or len(text) < self.min_content_length:
            return False
        if self.is_spam(text):
            return False
        if text.count('å¥½') > 5 or text.count('èµ') > 5:
            return False
        return True
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        if not text1 or not text2:
            return 0.0
        
        words1 = set(text1)
        words2 = set(text2)
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union)
    
    def deduplicate_advanced(self, data: List[Dict]) -> List[Dict]:
        if not data:
            return []
        
        unique_data = []
        seen_contents = []
        
        for item in data:
            content = item.get('cleaned_content', item.get('content', ''))
            
            is_duplicate = False
            for seen_content in seen_contents:
                similarity = self.calculate_similarity(content, seen_content)
                if similarity > self.similarity_threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_data.append(item)
                seen_contents.append(content)
        
        return unique_data
    
    def clean_data(self, data: List[Dict], min_valid_count: int = 2000) -> Tuple[List[Dict], Dict]:
        cleaned = []
        invalid_stats = {
            'too_short': 0,
            'spam': 0,
            'empty': 0,
            'other': 0
        }
        
        for item in data:
            content = item.get('content', '')
            
            if not content:
                invalid_stats['empty'] += 1
                continue
            
            if len(content) < self.min_content_length:
                invalid_stats['too_short'] += 1
                continue
            
            if self.is_spam(content):
                invalid_stats['spam'] += 1
                continue
            
            cleaned_content = self.clean_text(content)
            
            if not self.is_valid(cleaned_content):
                invalid_stats['other'] += 1
                continue
            
            item['cleaned_content'] = cleaned_content
            item['is_valid'] = True
            cleaned.append(item)
        
        cleaned = self.deduplicate_advanced(cleaned)
        
        stats = {
            'original_count': len(data),
            'cleaned_count': len(cleaned),
            'invalid_stats': invalid_stats,
            'valid_ratio': len(cleaned) / len(data) if data else 0,
            'meets_minimum': len(cleaned) >= min_valid_count
        }
        
        return cleaned, stats
    
    def merge_files(self, input_paths: List[str]) -> List[Dict]:
        all_data = []
        platform_counts = Counter()
        
        for path in input_paths:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    if isinstance(data, dict) and 'data' in data:
                        platform = data.get('platform', 'unknown')
                        platform_counts[platform] += len(data['data'])
                        all_data.extend(data['data'])
                    elif isinstance(data, list):
                        all_data.extend(data)
                        
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶ {path} å¤±è´¥: {e}")
        
        print(f"\nğŸ“Š æ•°æ®æºç»Ÿè®¡ï¼š")
        for platform, count in platform_counts.items():
            print(f"   {platform}: {count} æ¡")
        
        return all_data
    
    def save_to_json(self, data: List[Dict], stats: Dict, output_path: str):
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                "clean_time": datetime.now().isoformat(),
                "total_count": len(data),
                "stats": stats,
                "data": data
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… æ•°æ®æ¸…æ´—å®Œæˆ")
        print(f"{'='*50}")
        print(f"åŸå§‹æ•°æ®ï¼š{stats['original_count']} æ¡")
        print(f"æ¸…æ´—åæ•°æ®ï¼š{stats['cleaned_count']} æ¡")
        print(f"æœ‰æ•ˆç‡ï¼š{stats['valid_ratio']:.1%}")
        print(f"\næ— æ•ˆæ•°æ®ç»Ÿè®¡ï¼š")
        print(f"  å†…å®¹è¿‡çŸ­ï¼š{stats['invalid_stats']['too_short']} æ¡")
        print(f"  åƒåœ¾ä¿¡æ¯ï¼š{stats['invalid_stats']['spam']} æ¡")
        print(f"  å†…å®¹ä¸ºç©ºï¼š{stats['invalid_stats']['empty']} æ¡")
        print(f"  å…¶ä»–åŸå› ï¼š{stats['invalid_stats']['other']} æ¡")
        
        if not stats['meets_minimum']:
            print(f"\nâš ï¸  è­¦å‘Šï¼šæ¸…æ´—åæ•°æ®é‡ä¸è¶³2000æ¡ï¼Œå»ºè®®å¢åŠ æ•°æ®æŠ“å–")
        
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {output_path}")

def main():
    parser = argparse.ArgumentParser(description='æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--output', type=str, default='data/cleaned_data.json', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--min-valid-count', type=int, default=2000, help='æœ€å°æœ‰æ•ˆæ•°æ®é‡è¦æ±‚')
    
    args = parser.parse_args()
    
    cleaner = DataCleaner()
    
    if os.path.isdir(args.input):
        input_files = []
        for filename in os.listdir(args.input):
            if filename.endswith('.json'):
                input_files.append(os.path.join(args.input, filename))
        all_data = cleaner.merge_files(input_files)
    else:
        with open(args.input, 'r', encoding='utf-8') as f:
            data = json.load(f)
            all_data = data.get('data', []) if isinstance(data, dict) else data
    
    cleaned_data, stats = cleaner.clean_data(all_data, args.min_valid_count)
    cleaner.save_to_json(cleaned_data, stats, args.output)

if __name__ == '__main__':
    main()
