#!/usr/bin/env python3
"""
Semantic Search v1.5 - å¢å¼·ç‰ˆèªç¾©æœç´¢
å¼•å…¥ç›¸ä¼¼åº¦ç®—æ³•ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€æ¬Šé‡èª¿æ•´

æ”¹é€²ï¼š
1. è©é »-é€†æ–‡æª”é »ç‡ (TF-IDF) ç›¸ä¼¼åº¦
2. ä¸Šä¸‹æ–‡æ„ŸçŸ¥æœç´¢
3. è¨˜æ†¶æ¬Šé‡èˆ‡å¹´é½¡èª¿æ•´
4. èªç¾©ç­‰åƒ¹æ˜ å°„æ“´å±•

Usage:
    python semantic_search_v15.py "æš—ç‰©è³ªè¨ˆç®—"
    python semantic_search_v15.py "ARM èŠ¯ç‰‡" --context "æŠ€è¡“è¨è«–"
"""

import json
import re
import yaml
import math
from pathlib import Path
from typing import Optional, Set, List, Dict, Tuple
from collections import Counter
from datetime import datetime

SKILL_DIR = Path(__file__).parent.parent
CONFIG_FILE = SKILL_DIR / "config.yaml"
MEMORY_FILE = SKILL_DIR / "data" / "qst_memories.md"  # ç¨ç«‹å­˜å„²


def load_config() -> dict:
    """è¼‰å…¥é…ç½®"""
    if CONFIG_FILE.exists():
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    return {"tree": {}, "search": {}}


def load_memory() -> List[dict]:
    """
    è¼‰å…¥è¨˜æ†¶ä¸¦è§£æå…ƒæ•¸æ“š
    
    Returns:
        [{"content": "...", "weight": "N", "created": datetime, "category": "..."}]
    """
    memories = []
    if MEMORY_FILE.exists():
        with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
            entries = re.split(r'\n---+\n', content)
            
            for entry in entries:
                if not entry.strip():
                    continue
                
                # è§£ææ¬Šé‡æ¨™ç±¤
                weight = "N"
                weight_match = re.search(r'\[([CIN])\]', entry)
                if weight_match:
                    weight = weight_match.group(1)
                
                # è§£ææ—¥æœŸ
                created = datetime.now()
                date_match = re.search(r'(\d{4}-\d{2}-\d{2})', entry)
                if date_match:
                    try:
                        created = datetime.strptime(date_match.group(1), "%Y-%m-%d")
                    except:
                        pass
                
                # è§£æåˆ†é¡æ¨™ç±¤
                category = "General"
                cat_match = re.search(r'\[([A-Za-z_]+)\]', entry)
                if cat_match and cat_match.group(1) not in ['C', 'I', 'N']:
                    category = cat_match.group(1)
                
                memories.append({
                    "content": entry.strip(),
                    "weight": weight,
                    "created": created,
                    "category": category
                })
    
    return memories


def calculate_tf(tokens: List[str]) -> Dict[str, float]:
    """è¨ˆç®—è©é » (TF)"""
    counter = Counter(tokens)
    total = len(tokens)
    return {word: count / total for word, count in counter.items()}


def calculate_idf(documents: List[List[str]]) -> Dict[str, float]:
    """è¨ˆç®—é€†æ–‡æª”é »ç‡ (IDF)"""
    doc_count = len(documents)
    all_words = set(word for doc in documents for word in doc)
    
    idf = {}
    for word in all_words:
        docs_with_word = sum(1 for doc in documents if word in doc)
        idf[word] = math.log(doc_count / (1 + docs_with_word)) + 1
    
    return idf


def tokenize(text: str) -> List[str]:
    """
    åˆ†è©ï¼ˆç°¡åŒ–ç‰ˆï¼‰
    æ”¯æ´ä¸­è‹±æ–‡æ··åˆ
    """
    # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
    # åˆ†å‰²
    tokens = text.lower().split()
    # ä¸­æ–‡å–®å­—åˆ†å‰²
    chinese_tokens = []
    for token in tokens:
        if re.match(r'^[\u4e00-\u9fff]+$', token):
            chinese_tokens.extend(list(token))
        else:
            chinese_tokens.append(token)
    
    return chinese_tokens


def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
    common_words = set(vec1.keys()) & set(vec2.keys())
    
    if not common_words:
        return 0.0
    
    dot_product = sum(vec1[w] * vec2[w] for w in common_words)
    norm1 = math.sqrt(sum(v ** 2 for v in vec1.values()))
    norm2 = math.sqrt(sum(v ** 2 for v in vec2.values()))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)


def get_weight_multiplier(weight: str, age_days: float, config: dict) -> float:
    """
    æ ¹æ“šæ¬Šé‡å’Œå¹´é½¡è¨ˆç®—èª¿æ•´ä¿‚æ•¸
    
    Args:
        weight: è¨˜æ†¶æ¬Šé‡ (C/I/N)
        age_days: è¨˜æ†¶å¹´é½¡ï¼ˆå¤©ï¼‰
        config: é…ç½®
    
    Returns:
        èª¿æ•´ä¿‚æ•¸ (0.1 - 2.0)
    """
    decay = config.get("decay", {})
    
    if weight == "C":
        # Critical: æ°¸ä¸è¡°æ¸›ï¼Œå¢å¼·æ¬Šé‡
        return 2.0
    elif weight == "I":
        # Important: æ…¢è¡°æ¸›
        decay_rate = decay.get("important", 0.1)
        return max(0.5, 1.5 - age_days * decay_rate / 365)
    else:
        # Normal: å¿«è¡°æ¸›
        decay_rate = decay.get("normal", 0.5)
        return max(0.1, 1.0 - age_days * decay_rate / 30)


# èªç¾©ç­‰åƒ¹æ˜ å°„ï¼ˆæ“´å±•ç‰ˆï¼‰
SEMANTIC_EQUIVALENCES = {
    # å‹•æ¼«ç›¸é—œ
    "é‚£å€‹å‹•æ¼«": ["Dragon Ball", "é¾ç ", "æ‚Ÿç©º", "Dragon", "Ball"],
    "å‹•æ¼«": ["Dragon Ball", "é¾ç ", "anime", "å‹•ç•«"],
    
    # äººç‰©æŒ‡ä»£
    "ä»–": ["ç”¨æˆ¶", "ç§¦ç‹", "é™›ä¸‹", "King", "user"],
    "å¥¹": ["ç”¨æˆ¶", "user"],
    "ä½ ": ["ä¸ç›¸", "ææ–¯", "Li Si", "assistant"],
    "æœ•": ["ç§¦ç‹", "é™›ä¸‹", "King", "user"],
    
    # è¨˜æ†¶ç›¸é—œ
    "ä¹‹å‰èªªé": ["MEMORY.md", "è¨˜æ†¶", "memory", "ä¹‹å‰"],
    "è¨˜å¾—å—": ["è¨˜æ†¶", "memory", "recall"],
    "å–œæ­¡ä»€éº¼": ["åå¥½", "preference", "å–œæ­¡", "like"],
    
    # QST ç›¸é—œ
    "QSTæš—ç‰©è³ª": ["FSCA", "æš—ç‰©è³ª", "dark matter", "torsion", "å¹¾ä½•æ’“ç‡"],
    "æš—ç‰©è³ªç†è«–": ["FSCA", "dark matter", "QST", "torsion"],
    "ç²’å­ç‰©ç†": ["E8", "particle", "Standard Model", "æ¨™æº–æ¨¡å‹"],
    
    # æŠ€è¡“ç›¸é—œ
    "ARMèŠ¯ç‰‡": ["Tech", "èŠ¯ç‰‡", "chip", "CPU", "processor", "æŠ€è¡“"],
    "ARM èŠ¯ç‰‡": ["Tech", "èŠ¯ç‰‡", "chip", "CPU", "processor", "æŠ€è¡“"],
    "èŠ¯ç‰‡": ["chip", "CPU", "processor", "Tech", "æŠ€è¡“"],
    "æ¨¡å‹é…ç½®": ["Tech_Config_Model", "model", "é…ç½®", "GLM", "Gemini"],
    
    # é‚Šé˜²ç›¸é—œ
    "é‚Šé˜²": ["Border", "Security", "VPN", "firewall", "å®‰å…¨"],
    "ç›£æ§": ["Monitor", "monitor", "CPU", "memory", "ç³»çµ±"],
    
    # å¤–äº¤ç›¸é—œ
    "è«–å£‡": ["HKGBook", "forum", "å¤–äº¤", "å¸–å­"],
    "å¸–å­": ["post", "article", "HKGBook", "ç™¼è¡¨"],
}


def expand_semantic_query(query: str) -> Tuple[str, List[str]]:
    """
    æ“´å±•èªç¾©ç­‰åƒ¹è©
    
    Returns:
        (expanded_query, added_terms)
    """
    expanded = query
    added = []
    
    for phrase, equivalents in SEMANTIC_EQUIVALENCES.items():
        if phrase.lower() in query.lower():
            for eq in equivalents:
                if eq.lower() not in query.lower():
                    expanded += " " + eq
                    added.append(eq)
    
    return expanded, added


def expand_by_keywords(query: str, config: dict) -> Set[str]:
    """æ ¹æ“šé—œéµè©æ“´å±•åˆ†é¡"""
    categories = set()
    tree = config.get("tree", {})
    query_lower = query.lower()
    
    def scan_nodes(data: dict, path: str = ""):
        for name, node in data.items():
            keywords = node.get("keywords", [])
            for kw in keywords:
                if kw.lower() in query_lower:
                    categories.add(name)
                    categories.update(node.get("related", []))
                    break
            
            children = node.get("children", {})
            if children:
                scan_nodes(children, f"{path}/{name}" if path else name)
    
    scan_nodes(tree)
    
    return categories


def get_related_categories(primary: str, config: dict) -> Set[str]:
    """ç²å–ç›¸é—œåˆ†é¡"""
    related = set()
    tree = config.get("tree", {})
    
    def find_node(data: dict, target: str):
        for name, node in data.items():
            if name == target or target.lower() in name.lower():
                related.update(node.get("related", []))
            
            children = node.get("children", {})
            if children:
                find_node(children, target)
    
    find_node(tree, primary)
    related.add(primary)
    
    return related


def semantic_search_enhanced(
    query: str,
    context: Optional[List[str]] = None,
    expand: bool = True,
    min_relevance: float = 0.1
) -> dict:
    """
    å¢å¼·ç‰ˆèªç¾©æœç´¢
    
    Args:
        query: æœç´¢æŸ¥è©¢
        context: ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘å°è©±ï¼‰
        expand: æ˜¯å¦æ“´å±•ç›¸é—œåˆ†é¡
        min_relevance: æœ€å°ç›¸é—œåº¦é–¾å€¼
    
    Returns:
        {
            "primary": "FSCA",
            "path": ["QST", "Physics", "FSCA"],
            "related": ["QST_Computation", "QST_Audit"],
            "keywords": [...],
            "results": [{"content": "...", "score": 0.85, ...}],
            "stats": {...}
        }
    """
    config = load_config()
    memories = load_memory()
    
    # 1. æ“´å±•èªç¾©
    expanded_query, added_terms = expand_semantic_query(query)
    
    # 2. åˆä½µä¸Šä¸‹æ–‡
    if context:
        context_str = " ".join(context[-3:])
        expanded_query = f"{context_str} {expanded_query}"
    
    # 3. è­˜åˆ¥åˆ†é¡
    categories = expand_by_keywords(expanded_query, config)
    
    if not categories:
        categories = {"General"}
    
    primary = list(categories)[0]
    
    # 4. æ“´å±•ç›¸é—œåˆ†é¡
    if expand:
        related = get_related_categories(primary, config)
        categories.update(related)
    else:
        related = set()
    
    # 5. æ§‹å»ºæœç´¢ç´¢å¼•
    query_tokens = tokenize(expanded_query)
    query_tf = calculate_tf(query_tokens)
    
    # æ§‹å»ºæ–‡æª”é›†åˆ
    all_docs = [tokenize(m["content"]) for m in memories]
    idf = calculate_idf(all_docs + [query_tokens])
    
    # è¨ˆç®—æŸ¥è©¢ TF-IDF å‘é‡
    query_tfidf = {word: query_tf.get(word, 0) * idf.get(word, 1) for word in query_tokens}
    
    # 6. æœç´¢è¨˜æ†¶ä¸¦è¨ˆç®—ç›¸ä¼¼åº¦
    results = []
    
    for memory in memories:
        # åˆ†é¡éæ¿¾
        if memory["category"] not in categories and memory["category"] != "General":
            continue
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        mem_tokens = tokenize(memory["content"])
        mem_tf = calculate_tf(mem_tokens)
        mem_tfidf = {word: mem_tf.get(word, 0) * idf.get(word, 1) for word in mem_tokens}
        
        similarity = cosine_similarity(query_tfidf, mem_tfidf)
        
        # æ¬Šé‡èª¿æ•´
        age_days = (datetime.now() - memory["created"]).days
        weight_mult = get_weight_multiplier(memory["weight"], age_days, config)
        
        adjusted_score = similarity * weight_mult
        
        if adjusted_score >= min_relevance:
            results.append({
                "content": memory["content"],
                "score": round(adjusted_score, 3),
                "similarity": round(similarity, 3),
                "weight": memory["weight"],
                "weight_multiplier": round(weight_mult, 2),
                "category": memory["category"],
                "age_days": age_days
            })
    
    # 7. æ’åº
    results.sort(key=lambda x: x["score"], reverse=True)
    
    # 8. æ§‹å»ºè·¯å¾‘
    path = build_category_path(primary, config)
    
    return {
        "primary": primary,
        "path": path,
        "related": list(related - {primary}),
        "keywords": query_tokens[:10],
        "added_terms": added_terms,
        "results": results[:10],
        "count": len(results),
        "stats": {
            "total_memories": len(memories),
            "categories_searched": len(categories),
            "query_expanded": len(added_terms) > 0,
            "context_used": context is not None
        }
    }


def build_category_path(category: str, config: dict) -> List[str]:
    """æ§‹å»ºåˆ†é¡è·¯å¾‘"""
    tree = config.get("tree", {})
    path = []
    
    def search(data: dict, target: str, current_path: List[str]) -> bool:
        for name, node in data.items():
            new_path = current_path + [name]
            
            if name == target:
                path.extend(new_path)
                return True
            
            children = node.get("children", {})
            if children and search(children, target, new_path):
                return True
        
        return False
    
    search(tree, category, [])
    
    return path if path else [category]


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Enhanced Semantic Search v1.5")
    parser.add_argument("query", help="Search query")
    parser.add_argument("--context", "-c", help="Context (comma-separated)")
    parser.add_argument("--expand", action="store_true", help="Expand related categories")
    parser.add_argument("--min-relevance", type=float, default=0.1, help="Minimum relevance")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    context = args.context.split(",") if args.context else None
    
    result = semantic_search_enhanced(
        args.query,
        context=context,
        expand=args.expand,
        min_relevance=args.min_relevance
    )
    
    print(f"\nğŸ¯ Primary: {result['primary']}")
    print(f"ğŸ“ Path: {' â†’ '.join(result['path'])}")
    print(f"ğŸ”— Related: {', '.join(result['related'][:5])}")
    
    if result['added_terms']:
        print(f"â• Added terms: {', '.join(result['added_terms'])}")
    
    print(f"ğŸ“Š Found: {result['count']} memories")
    print(f"ğŸ“ˆ Stats: {result['stats']}")
    
    if args.verbose:
        print("\n--- Results ---")
        for i, r in enumerate(result['results'][:5], 1):
            print(f"\n[{i}] Score: {r['score']} (sim: {r['similarity']}, w: {r['weight']}x{r['weight_multiplier']})")
            print(f"Category: {r['category']} | Age: {r['age_days']} days")
            print(r['content'][:200] + "..." if len(r['content']) > 200 else r['content'])
