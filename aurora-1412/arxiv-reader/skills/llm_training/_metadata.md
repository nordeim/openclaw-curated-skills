# LLM Training & Alignment

本分类涵盖与 **大语言模型训练、微调和对齐** 相关的论文，包括但不限于：

- 预训练方法与架构改进（Scaling Laws, MoE, 新型 Attention 等）
- 监督微调（SFT）和指令微调（Instruction Tuning）
- RLHF / DPO / PPO 等对齐技术
- 数据工程（数据质量、数据配比、合成数据）
- 模型压缩与高效训练（量化、蒸馏、LoRA、Adapter）
- 长上下文训练与处理
- 安全对齐与价值对齐

**典型关键词**: pre-training, fine-tuning, RLHF, DPO, alignment, instruction tuning,
scaling law, LoRA, quantization, distillation, data curation, safety
