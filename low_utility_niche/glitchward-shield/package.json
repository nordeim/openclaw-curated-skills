{
  "name": "glitchward-llm-shield",
  "version": "1.0.0",
  "description": "Prompt injection detection API for AI agents. Scan prompts through 1,000+ patterns, 6-layer detection pipeline, 25+ attack categories, and 10+ language support.",
  "author": "Glitchward <lubos@glitchward.com>",
  "license": "MIT",
  "homepage": "https://glitchward.com/llm-shield",
  "repository": {
    "type": "git",
    "url": "https://github.com/glitchward/glitchward-llm-shield"
  },
  "keywords": [
    "prompt-injection",
    "llm-security",
    "ai-safety",
    "jailbreak-detection",
    "ai-firewall",
    "agent-security"
  ],
  "openclaw": {
    "skills": {
      "dependencies": {
        "tools": ["exec"],
        "binaries": ["curl", "jq"],
        "envVars": ["GLITCHWARD_SHIELD_TOKEN"]
      }
    }
  }
}
